# 3강. 머신러닝의 기초를 다집니다 - 수치 예측

## 03-1. 선형 회귀에 대해 알아보고 데이터를 준비합니다
: 선형 회귀(Linear Regression)는 머신러닝 알고리즘 중 가장 간단하면서도 딥러닝의 기초가 됨

### 1차 함수로 이해하는 선형 회귀
: 선형 회귀는 1차 함수로 표현 가능, 1차 함수의 기울기(slope)는 a이고, 절편(intercept)은 b
&ensp;&ensp;&ensp; **y = ax + b** 


- **선형 회귀는 기울기와 절편을 찾아 줍니다**
: 학교에서 배운 1차 함수의 경우 x에 따른 y값을 찾는데 집중한 반면, 선형 회귀에서는 이와 반대로 x, y가 주어졌을 때 기울기와 절편을 찾는데 집중한다.

- **그래프를 통해 선형 회귀의 문제 해결 과정을 이해해보자**
: 그래프에 찍힌 (x,y)좌표로 기울기와 절편을 추정하여 1차 함수를 추정해내는 것 -> **선형 회귀로 만든 모델**  
또한 이런 모델을 통해 새로운 점에 대한 예측을 할 수 있음.  
즉, 미리 준비한 입력(x: 3,4,5)과 타깃(y: 25,32,39)을 가지고 모델(y = 7x + 4)을 만든 다음 새 입력(6)에 대해 어떤 값을 예상한 것, 바로 이 과정이 선형 회귀 모델을 만들어 문제를 해결하는 과정이다

### 문제 해결을 위해 당뇨병 환자의 데이터 준비하기
- [colab ch03-1 code 참고](https://colab.research.google.com/drive/1VMAjL-f6nwBrEGIuOyVsbgZuvIILAYlD)

  


## 03-2. 경사 하강법으로 학습하는 방법을 알아봅니다

### 그래프로 경사 하강법의 의미를 알아봅니다
 - **선형 회귀와 경사 하강법의 관계를 이해합시다**
 : 산점도 그래프를 잘 표현하는 직선의 방정식을 찾는 것이 회귀 알고리즘의 목표였다. 경사 하강법이 바로 그 방법 중 하나이다. 
 **경사 하강법(gradient descent)** 이란, 모델이 데이터를 잘 표현할 수 있도록 기울기(변화율)를 사용하여 모델을 조금씩 조정하는 최적화 알고리즘이다. 이제 경사 하강법을 구현하기 위해 필요한 지식들을 알아보자.

 ### 예측값과 변화율에 대해 알아봅니다
 : 딥러닝 분야에서는 기울기 a를 가중치를 의미하는 ω 혹은 계수를 의미하는 θ 로, y는  ŷ (y-hat) 로 표기한다.   
 즉, y = ax + b -> ŷ = ωx + b로 모델을 이해하면 된다.   
 여기서 가중치 ω와 절편 b는 알고리즘이 찾은 규칙을 의미하고, ŷ는 우리가 예측한 값을 의미한다.

- **예측값이란 무엇일까요?**
: 입력과 출력 데이터를 통해 규칙을 발견하면 모델을 만들었다고 하는데, 그 모델에 대해 새로운 입력값을 넣으면 어떤 출력 값이 나오게 된다. 이 값이 모델을 통해 예측한 값, 즉 예측값이다.

### 예측값으로 올바른 모델 찾기
: 앞 식에서 찾아야 할 것은 훈련 데이터(x,y)에 잘 맞는  ω, b이다. ω, b 를 찾기 위한 방법은 아래와 같다

**훈련 데이터에 잘 맞는  ω, b를 찾는 방법**

1. 무작위로  ω, b를 정한다(무작위로 모델 만들기)
2. x에서 샘플 하나를 선택하여 ŷ을 계산한다(무작위로 모델 예측하기)
3. ŷ과 선택한 샘플의 진짜 y를 비교한다(예측한 값과 진짜 정답 비교하기)
4. ŷ이 y와 더 가까워지도록 ω, b를 조정한다(모델 조정하기)
5. 모든 샘플을 처리할 때까지 다시 2~4항목을 반복한다
[colab ch03-2 code 참고](https://colab.research.google.com/drive/1VMAjL-f6nwBrEGIuOyVsbgZuvIILAYlD)

### 변화율로 가중치 업데이트하기
- 지금부터는 x,y에 대한 방정식이 아닌 ω, ŷ에 대한 방정식으로 이해해보자.   
  - 선형 회귀의 목표 : y에 가까운 ŷ을 출력하는 모델(ω, b)을 찾아내는 것

- 따라서 지금부터는 ω와 b를 변화율로 업데이트 하는 방법을 알아볼 것 이다.
  [colab ch03-2 code 참고](https://colab.research.google.com/drive/1VMAjL-f6nwBrEGIuOyVsbgZuvIILAYlD)

: 이번 실습을 통해 y_hat을 증가시켜야 하는 상황을 가정하고 ω, b를 업데이트하는 방법에 대해 알아보았다.  
그러나 이 방법은 다음 두 가지 상황에 대해 적합하게 대처하지 못하므로 수동적인 방법이다.  

- y_hat이 y에 한참 미치치 못하는 값인 경우 ω, b를 더 큰 폭으로 수정할 수 없다
  - 앞에서 변화율만큼 수정을 했지만 특별한 기준을 정하기가 어렵기 때문
- y_hat이 y보다 커지면 y_hat을 감소시키지 못한다

: 이러한 문제를 해결하기 위해 다음 실습에서 ω, b를 더 능동적으로 업데이트 하는 방법인 오차 역전을 배워보자

### 오차 역전파로 가중치와 절편을 더 적절하게 업데이트합니다
: 오차 역전파(backpropagation)는 ŷ와 y의 차이를 이용하여 ω와 b를 업데이트합니다. 이 방법은 오차가 연이어 전파되는 모습으로 수행된다

[colab ch03-2 code 참고](https://colab.research.google.com/drive/1VMAjL-f6nwBrEGIuOyVsbgZuvIILAYlD)

**지금까지는 모델을 이렇게 만들었습니다**

1. ω, b를 임의의 값(1.0, 1.0)으로 초기화하고 훈련 데이터의 샘플을 하나씩 대입하여 y와 ŷ의 오차를 구함
2. 1에서 구한 오차를 ω, b의 변화율에 곱하고 이 값을 이용하여 ω, b를 업데이트 함
3. 만약 ŷ이 y보다 커지면 오차는 음수가 되어 자동으로 ω, b가 줄어드는 방향으로 업데이트 됨
4. 반대로  ŷ이 y보다 작으면 오차는 양수가 되고 ω, b는 더 커지도록 업데이트 됨



## 03-3. 손실 함수와 경사 하강법의 관계를 알아봅니다

: 경사 하강법을 좀 더 기술적으로 표현하면 '어떤 손실 함수가 정의되었을 때 손실 함수의 값이 최소가 되는 지점을 찾아가는 방법'이다

- 여기서 손실함수란 예상한 값과 실제 타깃값의 차이를 함수로 정의한 것을 말함

### 손실 함수의 정체를 파헤쳐봅니다

제곱 오차(Squared error)는 타깃값과 예측값을 뺀 다음 제곱한 것이다

SE = (y -  ŷ )^2

- 이때 제곱 오차가 최소가 되면 산점도 그래프를 가장 잘 표현한 직선이 그려짐

- 즉, 제곱 오차의 최솟값을 찾는 방법을 알면 모델을 쉽게 만들 수 있음

  - 제곱 오차 함수의 최솟값을 알아내는 방법

    - 기울기에 따라 함수의 값이 낮은 쪽으로 이동해야 함

    - 기울기를 구하려면 제곱 오차를 가중치나 절편에 대해 미분하면 됨

      [colab ch03-3 code 참고](https://colab.research.google.com/drive/1VMAjL-f6nwBrEGIuOyVsbgZuvIILAYlD)

**가중치에 대하여 제곱 오차 미분하기**

1. 제곱 오차를 가중치에 대하여 편미분하여 가중치에 대한 제곱 오차의 변화율을 구함

2. 1의 결과 값에 1/2를 곱하면 (y -  ŷ )x 의 결과가 나옴

   - 손실함수에 상수를 곱하거나 나누어도 최종 모델의 가중치나 절편에 영향을 주지않는 점을 이용

3. 가중치를 업데이트 함

   - ω에서 변화율을 빼는 방식을 선택

     : 손실 함수의 낮은 값으로 이동하고 싶기 때문

     

- 1,2,3의 과정을 거친 후 최종 식은  ω + (y -  ŷ)

- 즉 앞에서 오차 역전파를 알아보며 작성한 코드와 동일함을 알 수 있음

- 즉 아래의 코드를 가중치에 대하여 제곱 오차를 미분하는 것과 동일하게 사용할 수 있음

  ```python
  y_hat = x_i * w + b
  err = y_i - y_hat
  w_rate = x_i
  w = w + w_rate * err
  ```

  

**절편에 대하여 제곱 오차 미분하기**

- 앞서 같은 이유로 해당 식도 앞에서 작성한 코드와 정확히 일치

- 즉 아래의 코드를 절편에 대하여 제곱 오차를 미분하는 것과 동일하게 사용할 수 있음

  ```python
  err = y_i - y_hat
  b = b + 1*err
  ```

  

따라서 앞으로는 손실 함수에 대해 일일이 변화율의 값을 계산하는 대신 편미분을 사용하여 변화율을 계산함

- 변화율은 AI 분야에서 그레디언트(gradient)라고 부른다
- 앞으로 변화율은 모두 그레디언트로 언급할 것



## 03-4. 선형 회귀를 위한 뉴런을 만듭니다

: 해당 장에서 마지막으로 앞에서 만든 경사 하강법 알고리즘을 Neuron이라는 이름의 파이썬 클래스로 만들어 보겠다. 앞으로 배우게 될 알고리즘들은 이 클래스를 기반으로 확장하게 되므로 이 책에서 가장 중요한 작업을 하는 셈이다

[colab ch03-4 code 참고](https://colab.research.google.com/drive/1VMAjL-f6nwBrEGIuOyVsbgZuvIILAYlD)

1. _ _init_ _() 메서드 작성하기

   - w, b 선언 및 1.0(임의의 시작값) 초기화

     ```python
     def __init__(self):
     	self.w = 1.0
     	self.b = 1.0
     ```

     

2. 정방향 계산 만들기

   - 정방향 계산 : ŷ = wx + b

     ```python
     def forpass(self, x):
       y_hat = x * self.w + self.b
       return y_hat
     ```

     

3. 역방향 계산 만들기

   - 정방향 계산을 통해 얻어낸  ŷ과 y의 차이

   - 즉, 역방향 계산은 (-(y -  ŷ))으로 오차가 뉴런의 오른쪽 방향에서 왼쪽 방향으로 흐르는 것처럼 보임

   - 즉 오차가 역전파(backpropagation)된다

     ```python
     def backprop(self, x, err):
       w_grad = x * err # 가중치에 대한 그레이디언트를 계산
       b_grad = 1 * err # 절편에 대한 그레이디언트를 계산
       return w_grad, b_grad
     ```

     

4. 훈련을 위한 fit() 메서드 구현

   1. forpass() 메서드를 호출하여 ŷ 구하기
   2. 오차 계산 : ŷ - y
   3. Backprop() 메서드를 호출하여 가중치와 절편에 대한 그레이디언트 구하기
   4. 그레이디언트를 가중치와 절편에서 빼면 가중치와 절편 업데이트 완료

   : 1,2,3,4, 과정을 모든 훈련 샘플에 대해 수행하고(1 에포크에 해당) 적절한 가중치와 절편이 구해질만큼 반복하면 됨(약 100 에포크)

```python
def fit(self, x, y, epochs=100):
  for i in range(epochs): # 에포크만큼 반복
    for x_i, y_i in zip(x,y): # 모든 샘플에 대해 반복
      y_hat = self.forpass(x_i) # 정방향 계산
      err = -(y_i - y_hat) # 오차 계산
      w_grad, b_grad = self.backprop(x_i, err) # 역방향 계산
      self.w = w_grad # 가중치 업데이트
      self.b = b_grad # 절편 업데이트
```

