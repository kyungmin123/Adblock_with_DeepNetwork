{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import PIL\n",
    "import shutil\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ad_file_path='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\advertisement' # 일차적으로 수집한 광고 이미지\n",
    "os.makedirs(ad_file_path,exist_ok=True)\n",
    "ad_file_dir=os.listdir(ad_file_path)\n",
    "\n",
    "non_file_path='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\non_advertisement' # 일차적으로 수집한 비광고 이미지\n",
    "os.makedirs(non_file_path,exist_ok=True)\n",
    "non_file_dir=os.listdir(non_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 폴더의 파일 개수를 구하는 함수\n",
    "def num_of_files(dir):\n",
    "    x=0\n",
    "    for pack in os.walk(dir):\n",
    "        for f in pack[2]:\n",
    "            x+=1\n",
    "            \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset'\n",
    "os.makedirs(dataset,exist_ok=True)\n",
    "dataset_dir=os.listdir(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\training_set'\n",
    "os.makedirs(train,exist_ok=True)\n",
    "train_dir=os.listdir(train)\n",
    "\n",
    "valid='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\validation_set'\n",
    "os.makedirs(valid,exist_ok=True)\n",
    "valid_dir=os.listdir(valid)\n",
    "\n",
    "test='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\testing_set'\n",
    "os.makedirs(test,exist_ok=True)\n",
    "test_dir=os.listdir(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ad='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\training_set\\\\ad'\n",
    "os.makedirs(train_ad,exist_ok=True)\n",
    "train_ad_dir=os.listdir(train_ad)\n",
    "\n",
    "train_non='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\training_set\\\\non'\n",
    "os.makedirs(train_non,exist_ok=True)\n",
    "train_non_dir=os.listdir(train_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_ad='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\validation_set\\\\ad'\n",
    "os.makedirs(valid_ad,exist_ok=True)\n",
    "valid_ad_dir=os.listdir(valid_ad)\n",
    "\n",
    "valid_non='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\validation_set\\\\non'\n",
    "os.makedirs(valid_non,exist_ok=True)\n",
    "valid_non_dir=os.listdir(valid_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ad='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\testing_set\\\\ad'\n",
    "os.makedirs(test_ad,exist_ok=True)\n",
    "test_ad_dir=os.listdir(test_ad)\n",
    "\n",
    "test_non='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\dataset\\\\testing_set\\\\non'\n",
    "os.makedirs(test_non,exist_ok=True)\n",
    "test_non_dir=os.listdir(test_non)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "\n",
    "for file in train_ad_dir:\n",
    "    try:\n",
    "        os.rename(train_ad+'\\\\'+file,ad_file_path+'\\\\'+str(i)+'.jpg')\n",
    "        i+=1\n",
    "    except FileExistsError:\n",
    "        i+=1\n",
    "        continue\n",
    "        \n",
    "if(os.path.exists(ad_file_path) and num_of_files(ad_file_path)>0):\n",
    "    for file in test_ad_dir:\n",
    "        try:\n",
    "            os.rename(test_ad+'\\\\'+file,ad_file_path+'\\\\'+str(i)+'.jpg')\n",
    "            i+=1\n",
    "        except FileExistsError:\n",
    "            i+=1\n",
    "            continue      \n",
    "            \n",
    "if(os.path.exists(ad_file_path) and num_of_files(ad_file_path)>0):\n",
    "    for file in valid_ad_dir:\n",
    "        try:\n",
    "            os.rename(valid_ad+'\\\\'+file,ad_file_path+'\\\\'+str(i)+'.jpg')\n",
    "            i+=1\n",
    "        except FileExistsError:\n",
    "            i+=1\n",
    "            continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "존재한다면, train_ad 폴더와 test_ad 폴더와 valid_ad 폴더에 있는 파일들을 advertisement 폴더로 합병한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "j=1\n",
    "\n",
    "for file in train_non_dir:\n",
    "    try:\n",
    "        os.rename(train_non+'\\\\'+file,non_file_path+'\\\\'+str(j)+'.jpg')\n",
    "        j+=1\n",
    "    except FileExistsError:\n",
    "        j+=1\n",
    "        continue\n",
    "        \n",
    "if(os.path.exists(non_file_path) and num_of_files(non_file_path)>0):\n",
    "    for file in test_non_dir:\n",
    "        try:\n",
    "            os.rename(test_non+'\\\\'+file,non_file_path+'\\\\'+str(j)+'.jpg')\n",
    "            j+=1\n",
    "        except FileExistsError:\n",
    "            j+=1\n",
    "            continue      \n",
    "            \n",
    "if(os.path.exists(non_file_path) and num_of_files(non_file_path)>0):\n",
    "    for file in valid_non_dir:\n",
    "        try:\n",
    "            os.rename(valid_non+'\\\\'+file,non_file_path+'\\\\'+str(j)+'.jpg')\n",
    "            j+=1\n",
    "        except FileExistsError:\n",
    "            j+=1\n",
    "            continue "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "존재한다면, train_non 폴더와 test_non 폴더와 valid_non 폴더에 있는 파일들을 non_advertisement 폴더로 합병한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_ad='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\new_ad'\n",
    "os.makedirs(new_ad,exist_ok=True)\n",
    "new_ad_dir=os.listdir(new_ad)\n",
    "\n",
    "new_non='C:\\\\Users\\\\USER\\\\Desktop\\\\united_input\\\\new_non'\n",
    "os.makedirs(new_non,exist_ok=True)\n",
    "new_non_dir=os.listdir(new_non)\n",
    "\n",
    "\n",
    "if(os.path.exists(train_ad) and num_of_files(new_ad)>0):\n",
    "    i=num_of_files(train_ad)+1\n",
    "    for data in new_ad_dir:\n",
    "        try:\n",
    "            os.rename(new_ad+'\\\\'+data,train_ad+'\\\\'+str(i)+'.jpg')\n",
    "            i+=1\n",
    "        except FileNotFoundError:\n",
    "            print(\"FileNotFoundError!\")\n",
    "            break\n",
    "\n",
    "if(os.path.exists(train_non) and num_of_files(new_non)>0):\n",
    "    j=num_of_files(train_non)+1\n",
    "    for data in new_non_dir:\n",
    "        try:\n",
    "            os.rename(new_non+'\\\\'+data,train_non+'\\\\'+str(j)+'.jpg')\n",
    "            j+=1\n",
    "        except FileNotFoundError:\n",
    "            print(\"FileNotFoundError!\")\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "새 데이터를 추가하고자 할 때, new_ad , new_non 폴더에 담았다가 train_ad , train_non 폴더로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[339 177  75 529 154 245 439 230 161 150 404 140 471  76   2 281 413  12\n",
      " 297 315 535 374 488 484 390 121 345 214 306 537 256 430 328 263 256 244\n",
      " 500 464 167 468 504 139 507 366 279 191 257 432 218 174 245 131  82 316\n",
      " 329 390 316 538 450  27 456 411 310 378 240 165  58  51 370 433 165 191\n",
      "  92 494 505 340 428 516 529 421 410 138 179 497 451 467 251 458 491 413\n",
      " 281 503 448 203 413 490 105 362 476  48 561 452 133 254 370 439 233 480\n",
      "  32   2 177  15  90]\n",
      "[123 302  50 521 103 534 386 263 480 174 175 501 301 210  60  97 329 283\n",
      " 532 333 172  73 191 273 326  97 229 413 432 413 528 289 142 423 466 395\n",
      " 124 533 302 176 176 204 226 185 416 425 402   7 535 444  39 350  65 451\n",
      " 399  25 182  29 213 214 355 124 435 458 573 491 517 304 383 553  65 496\n",
      " 104 129 558 144 402 131  37  18  96 463   6 147 555 576  27 538 568 447\n",
      " 331 360 480  65 544 195 551 246 365  33 327  24 289  42 527 569  37 407\n",
      " 492 504 536 327 444 469 154]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(13)\n",
    "random_index_ad_arr=np.random.randint(1,num_of_files(ad_file_path)+1,size=num_of_files(ad_file_path)//5)\n",
    "random_index_non_arr=np.random.randint(1,num_of_files(non_file_path)+1,size=num_of_files(non_file_path)//5)\n",
    "\n",
    "print(random_index_ad_arr)\n",
    "print(random_index_non_arr)\n",
    "\n",
    "i=1\n",
    "if(os.path.exists(ad_file_path) and num_of_files(ad_file_path)>0):\n",
    "    for k in random_index_ad_arr:\n",
    "        try:\n",
    "            os.rename(ad_file_path+'\\\\'+str(k)+'.jpg',test_ad+'\\\\'+str(i)+'.jpg')\n",
    "            i+=1\n",
    "        except FileExistsError:\n",
    "            i=num_of_files(test_ad)+1\n",
    "            continue  \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            i=num_of_files(test_ad)+1\n",
    "            continue\n",
    "            \n",
    "j=1\n",
    "if(os.path.exists(non_file_path) and num_of_files(non_file_path)>0):\n",
    "    for k in random_index_non_arr:\n",
    "        try:\n",
    "            os.rename(non_file_path+'\\\\'+str(k)+'.jpg',test_non+'\\\\'+str(j)+'.jpg')\n",
    "            j+=1\n",
    "        except FileExistsError:\n",
    "            j=num_of_files(test_non)+1\n",
    "            continue \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            j=num_of_files(test_non)+1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advertisement 폴더와 non_advertisement 폴더에서 각 파일 수의 1/5만큼 난수를 생성하고,\n",
    "그 난수에 해당하는 ad 파일과 non 파일을 각각 test_ad 폴더와 test_non 폴더로 이동시켰다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[458 208 313 261  49 369 121 420 189  99 242 203 329 111 448 429 318 254\n",
      " 234 254 247 123 305 239 357  85 316 149 376 205 185 170 134 230 275 455\n",
      "  51 441 237  49 310 262  81 292 369 249 418 269 241 339 272 391 347  29\n",
      " 138 465 366 385 134 326 236 442 119 389 333 287 287 461 231 410 417 251\n",
      " 219 123 161 113  64 410 208 182 323 122  72  34   9  59 435 104 281 282\n",
      " 127 108  65]\n",
      "[154 221 207 304 424   6 172  19 296 246 255 353 133 163  51 353 314 370\n",
      " 145 228   2 308   2  28 162 247  33 288 195 343 331 222 346 338  68   2\n",
      "  66 208 162  50  34 268 268 206 247 246 184 230 421 471 126  82 458 286\n",
      " 108 381   4  95  39 431 349 409 194 298 258 221  81 190 238 282 121 420\n",
      "  11 330  69  29 145  35  42   1 185 159 463   2 335 194  37 119 251 377\n",
      " 446 182 385 260 264]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(21)\n",
    "random_index_ad_arr=np.random.randint(1,num_of_files(ad_file_path)+1,size=num_of_files(ad_file_path)//5)\n",
    "random_index_non_arr=np.random.randint(1,num_of_files(non_file_path)+1,size=num_of_files(non_file_path)//5)\n",
    "\n",
    "print(random_index_ad_arr)\n",
    "print(random_index_non_arr)\n",
    "\n",
    "i=1\n",
    "if(os.path.exists(ad_file_path) and num_of_files(ad_file_path)>0):\n",
    "    for k in random_index_ad_arr:\n",
    "        try:\n",
    "            os.rename(ad_file_path+'\\\\'+str(k)+'.jpg',valid_ad+'\\\\'+str(i)+'.jpg')\n",
    "            i+=1\n",
    "        except FileExistsError:\n",
    "            i=num_of_files(valid_ad)+1\n",
    "            continue  \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            i+=1\n",
    "            continue\n",
    "            \n",
    "j=1\n",
    "if(os.path.exists(non_file_path) and num_of_files(non_file_path)>0):\n",
    "    for k in random_index_non_arr:\n",
    "        try:\n",
    "            os.rename(non_file_path+'\\\\'+str(k)+'.jpg',valid_non+'\\\\'+str(j)+'.jpg')\n",
    "            j+=1\n",
    "        except FileExistsError:\n",
    "            j=num_of_files(valid_non)+1\n",
    "            continue \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            j+=1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advertisement 폴더와 non_advertisement 폴더에서 각 파일 수의 1/5만큼 난수를 생성하고, 그 난수에 해당하는 ad 파일과 non 파일을 각각 valid_ad 폴더와 valid_non 폴더로 이동시켰다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=1\n",
    "n=1\n",
    "if(os.path.exists(ad_file_path) and num_of_files(ad_file_path)>0):\n",
    "    while num_of_files(ad_file_path)>0:\n",
    "        try:\n",
    "            os.rename(ad_file_path+'\\\\'+str(i)+'.jpg',train_ad+'\\\\'+str(n)+'.jpg')\n",
    "            i+=1\n",
    "            n+=1\n",
    "\n",
    "        except FileExistsError:\n",
    "            n=num_of_files(train_ad)+1\n",
    "            continue  \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            i+=1\n",
    "            continue\n",
    "            \n",
    "j=1\n",
    "m=1\n",
    "if(os.path.exists(non_file_path) and num_of_files(non_file_path)>0):\n",
    "    while num_of_files(non_file_path)>0:\n",
    "        try:\n",
    "            os.rename(non_file_path+'\\\\'+str(j)+'.jpg',train_non+'\\\\'+str(m)+'.jpg')\n",
    "            j+=1\n",
    "            m+=1\n",
    "            \n",
    "        except FileExistsError:\n",
    "            m=num_of_files(train_non)+1\n",
    "            continue \n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            j+=1\n",
    "            continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advertisement 폴더와 non_advertisement 폴더에 남아 있는 ad , non 파일을 각각 train_ad 폴더와 train_non 폴더로 이동시켰다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "if(os.path.exists(ad_file_path) and num_of_files(ad_file_path)==0):\n",
    "    if(num_of_files(dataset)>0):\n",
    "        os.rmdir(ad_file_path)\n",
    "        \n",
    "if(os.path.exists(non_file_path) and num_of_files(non_file_path)==0):\n",
    "    if(num_of_files(dataset)>0):\n",
    "        os.rmdir(non_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "advertisement 폴더와 non_advertisement 폴더가 비었고, dataset 폴더에 파일이 존재한다면,\n",
    "advertisement 폴더와 non_advertisement 폴더를 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout, Flatten, Dense\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras import optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILTER_SIZE=3\n",
    "NUM_FILTERS=25\n",
    "BATCH_SIZE=20\n",
    "MAXPOOL_SIZE=2\n",
    "STEPS_PER_EPOCH=500//BATCH_SIZE\n",
    "EPOCHS=50\n",
    "INPUT_SIZE=64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),\n",
    "                input_shape=(INPUT_SIZE,INPUT_SIZE,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(MAXPOOL_SIZE,MAXPOOL_SIZE)))\n",
    "\n",
    "model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),\n",
    "                input_shape=(INPUT_SIZE,INPUT_SIZE,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(MAXPOOL_SIZE,MAXPOOL_SIZE)))\n",
    "\n",
    "model.add(Conv2D(NUM_FILTERS, (FILTER_SIZE, FILTER_SIZE),\n",
    "                input_shape=(INPUT_SIZE,INPUT_SIZE,3),activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(MAXPOOL_SIZE,MAXPOOL_SIZE)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(units=128,activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 807 images belonging to 2 classes.\n",
      "Found 135 images belonging to 2 classes.\n",
      "Found 201 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "im_generator=tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "ITERATIONS=1\n",
    "images=[]\n",
    "\n",
    "train_generator=im_generator.flow_from_directory(train,target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                    class_mode='binary',batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True,seed=24)\n",
    "\n",
    "valid_generator=im_generator.flow_from_directory(valid,target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                    class_mode='binary',batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True,seed=24)\n",
    "\n",
    "test_generator=im_generator.flow_from_directory(test,target_size=(INPUT_SIZE,INPUT_SIZE),\n",
    "                                    class_mode='binary',batch_size=BATCH_SIZE,\n",
    "                                    shuffle=True,seed=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ad': 0, 'non': 1}\n"
     ]
    }
   ],
   "source": [
    "print(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  warnings.warn('`Model.fit_generator` is deprecated and '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "25/25 [==============================] - 7s 233ms/step - loss: 0.6990 - accuracy: 0.5161 - val_loss: 0.6891 - val_accuracy: 0.5037\n",
      "Epoch 2/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 0.6812 - accuracy: 0.5299 - val_loss: 0.6841 - val_accuracy: 0.5185\n",
      "Epoch 3/50\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 0.6639 - accuracy: 0.5719 - val_loss: 0.6727 - val_accuracy: 0.5556\n",
      "Epoch 4/50\n",
      "25/25 [==============================] - 5s 194ms/step - loss: 0.6656 - accuracy: 0.6067 - val_loss: 0.6392 - val_accuracy: 0.6148\n",
      "Epoch 5/50\n",
      "25/25 [==============================] - 5s 180ms/step - loss: 0.6365 - accuracy: 0.6700 - val_loss: 0.6818 - val_accuracy: 0.6074\n",
      "Epoch 6/50\n",
      "25/25 [==============================] - 5s 190ms/step - loss: 0.5700 - accuracy: 0.7103 - val_loss: 0.5180 - val_accuracy: 0.7852\n",
      "Epoch 7/50\n",
      "25/25 [==============================] - 5s 200ms/step - loss: 0.4953 - accuracy: 0.7686 - val_loss: 0.4720 - val_accuracy: 0.7630\n",
      "Epoch 8/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.4357 - accuracy: 0.7835 - val_loss: 0.4689 - val_accuracy: 0.7926\n",
      "Epoch 9/50\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 0.4627 - accuracy: 0.8179 - val_loss: 0.5287 - val_accuracy: 0.7037\n",
      "Epoch 10/50\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 0.4365 - accuracy: 0.8037 - val_loss: 0.5070 - val_accuracy: 0.7259\n",
      "Epoch 11/50\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 0.4912 - accuracy: 0.7532 - val_loss: 0.4006 - val_accuracy: 0.8519\n",
      "Epoch 12/50\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 0.3791 - accuracy: 0.8384 - val_loss: 0.5407 - val_accuracy: 0.7259\n",
      "Epoch 13/50\n",
      "25/25 [==============================] - 5s 200ms/step - loss: 0.4190 - accuracy: 0.8305 - val_loss: 0.3874 - val_accuracy: 0.8148\n",
      "Epoch 14/50\n",
      "25/25 [==============================] - 4s 173ms/step - loss: 0.3750 - accuracy: 0.8243 - val_loss: 0.5000 - val_accuracy: 0.7704\n",
      "Epoch 15/50\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 0.3964 - accuracy: 0.8192 - val_loss: 0.3874 - val_accuracy: 0.8667\n",
      "Epoch 16/50\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 0.3130 - accuracy: 0.8748 - val_loss: 0.4833 - val_accuracy: 0.7778\n",
      "Epoch 17/50\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 0.3526 - accuracy: 0.8599 - val_loss: 0.3836 - val_accuracy: 0.8222\n",
      "Epoch 18/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.3098 - accuracy: 0.8625 - val_loss: 0.3862 - val_accuracy: 0.8444\n",
      "Epoch 19/50\n",
      "25/25 [==============================] - 4s 175ms/step - loss: 0.3174 - accuracy: 0.8520 - val_loss: 0.4181 - val_accuracy: 0.8296\n",
      "Epoch 20/50\n",
      "25/25 [==============================] - 5s 177ms/step - loss: 0.2809 - accuracy: 0.8769 - val_loss: 0.7185 - val_accuracy: 0.6296\n",
      "Epoch 21/50\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 0.4474 - accuracy: 0.7904 - val_loss: 0.4197 - val_accuracy: 0.7852\n",
      "Epoch 22/50\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.2530 - accuracy: 0.9034 - val_loss: 0.4392 - val_accuracy: 0.8148\n",
      "Epoch 23/50\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 0.2220 - accuracy: 0.8962 - val_loss: 0.3905 - val_accuracy: 0.8222\n",
      "Epoch 24/50\n",
      "25/25 [==============================] - 4s 179ms/step - loss: 0.2388 - accuracy: 0.8951 - val_loss: 0.4531 - val_accuracy: 0.8148\n",
      "Epoch 25/50\n",
      "25/25 [==============================] - 5s 182ms/step - loss: 0.2289 - accuracy: 0.8996 - val_loss: 0.4032 - val_accuracy: 0.8593\n",
      "Epoch 26/50\n",
      "25/25 [==============================] - 4s 159ms/step - loss: 0.2014 - accuracy: 0.9237 - val_loss: 0.4178 - val_accuracy: 0.8222\n",
      "Epoch 27/50\n",
      "25/25 [==============================] - 5s 188ms/step - loss: 0.1552 - accuracy: 0.9441 - val_loss: 0.4052 - val_accuracy: 0.8074\n",
      "Epoch 28/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.1958 - accuracy: 0.8988 - val_loss: 0.4107 - val_accuracy: 0.8296\n",
      "Epoch 29/50\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.2069 - accuracy: 0.9204 - val_loss: 0.4315 - val_accuracy: 0.8370\n",
      "Epoch 30/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.2069 - accuracy: 0.9267 - val_loss: 0.4043 - val_accuracy: 0.8222\n",
      "Epoch 31/50\n",
      "25/25 [==============================] - 4s 175ms/step - loss: 0.1477 - accuracy: 0.9569 - val_loss: 0.4158 - val_accuracy: 0.8370\n",
      "Epoch 32/50\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 0.1231 - accuracy: 0.9571 - val_loss: 0.4315 - val_accuracy: 0.8222\n",
      "Epoch 33/50\n",
      "25/25 [==============================] - 5s 178ms/step - loss: 0.1082 - accuracy: 0.9510 - val_loss: 0.4469 - val_accuracy: 0.8444\n",
      "Epoch 34/50\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 0.1082 - accuracy: 0.9606 - val_loss: 0.5176 - val_accuracy: 0.8519\n",
      "Epoch 35/50\n",
      "25/25 [==============================] - 4s 180ms/step - loss: 0.0828 - accuracy: 0.9629 - val_loss: 0.5072 - val_accuracy: 0.8370\n",
      "Epoch 36/50\n",
      "25/25 [==============================] - 5s 183ms/step - loss: 0.0873 - accuracy: 0.9809 - val_loss: 0.4935 - val_accuracy: 0.8074\n",
      "Epoch 37/50\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.1072 - accuracy: 0.9541 - val_loss: 0.4503 - val_accuracy: 0.8370\n",
      "Epoch 38/50\n",
      "25/25 [==============================] - 4s 162ms/step - loss: 0.0722 - accuracy: 0.9731 - val_loss: 0.4765 - val_accuracy: 0.8222\n",
      "Epoch 39/50\n",
      "25/25 [==============================] - 4s 177ms/step - loss: 0.0892 - accuracy: 0.9615 - val_loss: 0.7142 - val_accuracy: 0.8370\n",
      "Epoch 40/50\n",
      "25/25 [==============================] - 4s 175ms/step - loss: 0.0814 - accuracy: 0.9703 - val_loss: 0.5459 - val_accuracy: 0.8444\n",
      "Epoch 41/50\n",
      "25/25 [==============================] - 4s 176ms/step - loss: 0.0634 - accuracy: 0.9815 - val_loss: 0.5351 - val_accuracy: 0.8000\n",
      "Epoch 42/50\n",
      "25/25 [==============================] - 4s 172ms/step - loss: 0.0873 - accuracy: 0.9548 - val_loss: 0.6127 - val_accuracy: 0.8222\n",
      "Epoch 43/50\n",
      "25/25 [==============================] - 5s 191ms/step - loss: 0.0560 - accuracy: 0.9796 - val_loss: 0.6588 - val_accuracy: 0.8222\n",
      "Epoch 44/50\n",
      "25/25 [==============================] - 5s 184ms/step - loss: 0.0414 - accuracy: 0.9921 - val_loss: 0.5971 - val_accuracy: 0.8148\n",
      "Epoch 45/50\n",
      "25/25 [==============================] - 4s 168ms/step - loss: 0.0478 - accuracy: 0.9895 - val_loss: 0.5272 - val_accuracy: 0.8444\n",
      "Epoch 46/50\n",
      "25/25 [==============================] - 4s 170ms/step - loss: 0.0451 - accuracy: 0.9948 - val_loss: 0.5610 - val_accuracy: 0.8222\n",
      "Epoch 47/50\n",
      "25/25 [==============================] - 5s 193ms/step - loss: 0.0314 - accuracy: 0.9952 - val_loss: 0.5153 - val_accuracy: 0.8370\n",
      "Epoch 48/50\n",
      "25/25 [==============================] - 4s 171ms/step - loss: 0.0390 - accuracy: 0.9876 - val_loss: 0.6670 - val_accuracy: 0.8370\n",
      "Epoch 49/50\n",
      "25/25 [==============================] - 4s 175ms/step - loss: 0.0380 - accuracy: 0.9886 - val_loss: 0.6056 - val_accuracy: 0.8148\n",
      "Epoch 50/50\n",
      "25/25 [==============================] - 4s 178ms/step - loss: 0.0253 - accuracy: 0.9971 - val_loss: 0.6065 - val_accuracy: 0.7926\n"
     ]
    }
   ],
   "source": [
    "history=model.fit_generator(train_generator,steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                   validation_data=valid_generator,epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련 결과"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
